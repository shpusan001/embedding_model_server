from fastapi import FastAPI, Query
from transformers import AutoTokenizer, AutoModel
import torch
import time

app = FastAPI()

MODEL_PATH = "./local_models/labse/models--sentence-transformers--LaBSE/snapshots/836121a0533e5664b21c7aacc5d22951f2b8b25b"  # âœ… ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ

print("ğŸŸ¡ ëª¨ë¸ ë¡œë”© ì‹œì‘...")

start = time.time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
print(f"âœ… Tokenizer ë¡œë”© ì™„ë£Œ ({time.time() - start:.2f}s)")

start = time.time()
model = AutoModel.from_pretrained(MODEL_PATH)
print(f"âœ… Model ë¡œë”© ì™„ë£Œ ({time.time() - start:.2f}s)")

@app.get("/")
async def home():
    return {"message": "Hello World"}

@app.get("/embed")
async def embed(text: str = Query(..., min_length=1)):
    try:
        print(f"ğŸ“© ìš”ì²­ ë„ì°©: {text}")
        inputs = tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
            pooled = outputs.last_hidden_state.mean(dim=1)
        print("âœ… ì„ë² ë”© ì™„ë£Œ")
        return {"embedding": pooled[0].tolist()}
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": str(e)}
